{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "from astropy.coordinates import Angle\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import ascii\n",
    "from astropy.table import setdiff, Table, unique, vstack\n",
    "import numpy as np\n",
    "%run Crossmatch.ipynb\n",
    "%run Utility.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## NEW CELL WITH SEQUENTIAL AVERAGING ##\n",
    "%run Crossmatch.ipynb\n",
    "total_duplicates = Table()\n",
    "\n",
    "# Path for aperture photometry based PSF-modelled catalogs\n",
    "path = '/home/yobd/Documents/Photometry_Output/Zero_Point_and_Color_Term_Aper_Model_Individually/'\n",
    "\n",
    "averaged = ascii.read('/home/yobd/Documents/Photometry_Output/Zero_Point_and_Color_Term_Aper_Model_Individually/Chihiro.dat')\n",
    "\n",
    "# Encodes the path of the dataset to be used by other OS methods\n",
    "\n",
    "folder = ['Clarisse', 'Fio', 'Kiki', 'Nausicaa', 'Ponyo', 'San', 'Satsuki', 'Sheeta', 'Sophie']\n",
    "\n",
    "# Cycles through the folder where the catalogs are stored - They will be cross-matched with each other in order to find all the duplicates in our images\n",
    "\n",
    "for item in folder:\n",
    "    \n",
    "    # The name of the current catalog\n",
    "    \n",
    "    item_name = item\n",
    "    \n",
    "    item = path + item + '.dat' # To comply with the func() function parameter\n",
    "    \n",
    "    # Opens the current catalog as an astropy.Table object\n",
    "    \n",
    "    cat = ascii.read(item)\n",
    "    \n",
    "    # The astropy.Table object containing the cross-matches between the current catalog and the second catalog being iterated\n",
    "    \n",
    "    temp_duplicates = cross_match(averaged, 'ra', 'dec', cat, 'ra', 'dec', 3, sep = True)\n",
    "    \n",
    "    # Only perform the following actions if there are cross-matches, otherwise it will give an error (i.e., when cross-matching catalogs at opposite ends of the instrument)\n",
    "    \n",
    "    if len(temp_duplicates) != 0:\n",
    "        \n",
    "        # Resets the columns of the current catalog back to their original names\n",
    "        \n",
    "        temp_duplicates = reset_main_catalog_columns(averaged, temp_duplicates)\n",
    "        \n",
    "        temp_duplicates.sort('d2d')\n",
    "        \n",
    "        temp_duplicates = unique(temp_duplicates, keys = ['ra_2', 'dec_2'], keep = 'first')\n",
    "        \n",
    "        singles = setdiff(averaged, temp_duplicates, keys = ['ra', 'dec'])\n",
    "        \n",
    "        cat['ra_2'] = cat['ra']\n",
    "        \n",
    "        cat['dec_2'] = cat['dec']\n",
    "        \n",
    "        singles_cat = setdiff(cat, temp_duplicates, keys = ['ra_2', 'dec_2'])\n",
    "        \n",
    "        remove_new_columns(averaged, singles_cat)\n",
    "        \n",
    "        # Adds the duplicates from the current catalog - second catalog pair onto the astropy.Table containing all the duplicates across all detectors\n",
    "        \n",
    "        total_duplicates = vstack([total_duplicates, temp_duplicates])\n",
    "        \n",
    "        temp_duplicates['ra'] = (temp_duplicates['ra'] + temp_duplicates['ra_2']) / 2\n",
    "        \n",
    "        temp_duplicates['dec'] = (temp_duplicates['dec'] + temp_duplicates['dec_2']) / 2\n",
    "        \n",
    "        temp_duplicates['I_MAG_SUBARU'] = (temp_duplicates['I_MAG_SUBARU'] + temp_duplicates['I_MAG_SUBARU_2']) / 2\n",
    "        \n",
    "        temp_duplicates['I_MAG_SUBARU_ERR'] = np.sqrt(0.25 * temp_duplicates['I_MAG_SUBARU_ERR'] * temp_duplicates['I_MAG_SUBARU_ERR'] + 0.25 * temp_duplicates['I_MAG_SUBARU_ERR_2'] * temp_duplicates['I_MAG_SUBARU_ERR_2'])\n",
    "        \n",
    "        temp_duplicates['I_FLUX_SUBARU'] = (temp_duplicates['I_FLUX_SUBARU'] + temp_duplicates['I_FLUX_SUBARU_2']) / 2\n",
    "        \n",
    "        temp_duplicates['I_FLUX_SUBARU_ERR'] = np.sqrt(0.25 * temp_duplicates['I_FLUX_SUBARU_ERR'] * temp_duplicates['I_FLUX_SUBARU_ERR'] + 0.25 * temp_duplicates['I_FLUX_SUBARU_ERR_2'] * temp_duplicates['I_FLUX_SUBARU_ERR_2'])\n",
    "        \n",
    "        temp_duplicates['I_FLUX_RADIUS_SUBARU'] = (temp_duplicates['I_FLUX_RADIUS_SUBARU'] + temp_duplicates['I_FLUX_RADIUS_SUBARU_2']) / 2\n",
    "        \n",
    "        temp_duplicates['ELONGATION'] = (temp_duplicates['ELONGATION'] + temp_duplicates['ELONGATION_2']) / 2\n",
    "        \n",
    "        temp_duplicates['ELLIPTICITY'] = (temp_duplicates['ELLIPTICITY'] + temp_duplicates['ELLIPTICITY_2']) / 2\n",
    "        \n",
    "        temp_duplicates['I_SNR_WIN_SUBARU'] = (temp_duplicates['I_SNR_WIN_SUBARU'] + temp_duplicates['I_SNR_WIN_SUBARU_2']) / 2\n",
    "        \n",
    "        #temp_duplicates['FLAGS'] = temp_duplicates['FLAGS'] + temp_duplicates['FLAGS_2']\n",
    "        \n",
    "        #temp_duplicates['FLAGS_MODEL'] = temp_duplicates['FLAGS_MODEL'] + temp_duplicates['FLAGS_MODEL_2']\n",
    "        \n",
    "        #temp_duplicates['I_FLUX_APER_SUBARU'] = (temp_duplicates['I_FLUX_APER_SUBARU'] + temp_duplicates['I_FLUX_APER_SUBARU_2']) / 2\n",
    "        \n",
    "        #temp_duplicates['I_FLUX_APER_SUBARU_ERR'] = np.sqrt(0.25 * temp_duplicates['I_FLUX_APER_SUBARU_ERR'] * temp_duplicates['I_FLUX_APER_SUBARU_ERR'] + 0.25 * temp_duplicates['I_FLUX_APER_SUBARU_ERR_2'] * temp_duplicates['I_FLUX_APER_SUBARU_ERR_2'])\n",
    "        \n",
    "        # Removes the columns from the cross-matched second catalog\n",
    "        \n",
    "        remove_new_columns(averaged, temp_duplicates)\n",
    "        \n",
    "        averaged = vstack([singles, temp_duplicates])\n",
    "        \n",
    "        averaged = vstack([averaged, singles_cat])\n",
    "\n",
    "\n",
    "# Path for aperture photometry based PSF-modelled catalogs\n",
    "ascii.write(averaged, '/home/yobd/Documents/Photometry_Output/Full_Catalog_ZP_CT_Aper_PSF.dat', format = 'csv', overwrite = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yobd/.local/lib/python3.7/site-packages/astropy/table/table.py:2020: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  keep_mask[row_specifier] = False\n"
     ]
    }
   ],
   "source": [
    "## The duplicates code for the H-Alpha catalog using last cell's method\n",
    "\n",
    "%run Crossmatch.ipynb\n",
    "total_duplicates = Table()\n",
    "\n",
    "# The path where the dataset is located\n",
    "\n",
    "path = '/home/yobd/Documents/AstroAliSciObj/N-A-L656/MINAREA_4_THRESH_2_dot_7_SNR_20_SATUR_LEVEL_B4_PSFEX_30000_SMP_VAR_0_dot_2_SEEING_6_dot_7_7_MAG_PSF/Catalogs_Finished/'\n",
    "\n",
    "averaged = ascii.read('/home/yobd/Documents/AstroAliSciObj/N-A-L656/MINAREA_4_THRESH_2_dot_7_SNR_20_SATUR_LEVEL_B4_PSFEX_30000_SMP_VAR_0_dot_2_SEEING_6_dot_7_7_MAG_PSF/Catalogs_Finished/N-A-L656_chihiro_stacked_tmp.dat')\n",
    "\n",
    "# Encodes the path of the dataset to be used by other OS methods\n",
    "\n",
    "folder = ['N-A-L656_clarisse_stacked_tmp', 'N-A-L656_fio_stacked_tmp', 'N-A-L656_kiki_stacked_tmp', 'N-A-L656_nausicaa_stacked_tmp', 'N-A-L656_ponyo_stacked_tmp', 'N-A-L656_san_stacked_tmp', 'N-A-L656_satsuki_stacked_tmp', 'N-A-L656_sheeta_stacked_tmp', 'N-A-L656_sophie_stacked_tmp']\n",
    "\n",
    "# Cycles through the folder where the catalogs are stored - They will be cross-matched with each other in order to find all the duplicates in our images\n",
    "\n",
    "for item in folder:\n",
    "    \n",
    "    # The name of the current catalog\n",
    "    \n",
    "    item_name = item\n",
    "    \n",
    "    item = path + item + '.dat' # To comply with the func() function parameter\n",
    "    \n",
    "    # Opens the current catalog as an astropy.Table object\n",
    "    \n",
    "    cat = ascii.read(item)\n",
    "    \n",
    "    # The astropy.Table object containing the cross-matches between the current catalog and the second catalog being iterated\n",
    "    \n",
    "    temp_duplicates = cross_match(averaged, 'ra', 'dec', cat, 'ra', 'dec', 3, sep = True)\n",
    "    \n",
    "    # Only perform the following actions if there are cross-matches, otherwise it will give an error (i.e., when cross-matching catalogs at opposite ends of the instrument)\n",
    "    \n",
    "    if len(temp_duplicates) != 0:\n",
    "        \n",
    "        # Resets the columns of the current catalog back to their original names\n",
    "        \n",
    "        temp_duplicates = reset_main_catalog_columns(averaged, temp_duplicates)\n",
    "        \n",
    "        temp_duplicates.sort('d2d')\n",
    "        \n",
    "        temp_duplicates = unique(temp_duplicates, keys = ['ra_2', 'dec_2'], keep = 'first')\n",
    "        \n",
    "        singles = setdiff(averaged, temp_duplicates, keys = ['ra', 'dec'])\n",
    "        \n",
    "        cat['ra_2'] = cat['ra']\n",
    "        \n",
    "        cat['dec_2'] = cat['dec']\n",
    "        \n",
    "        singles_cat = setdiff(cat, temp_duplicates, keys = ['ra_2', 'dec_2'])\n",
    "        \n",
    "        remove_new_columns(averaged, singles_cat)\n",
    "        \n",
    "        # Adds the duplicates from the current catalog - second catalog pair onto the astropy.Table containing all the duplicates across all detectors\n",
    "        \n",
    "        total_duplicates = vstack([total_duplicates, temp_duplicates])\n",
    "        \n",
    "        temp_duplicates['ra'] = (temp_duplicates['ra'] + temp_duplicates['ra_2']) / 2\n",
    "        \n",
    "        temp_duplicates['dec'] = (temp_duplicates['dec'] + temp_duplicates['dec_2']) / 2\n",
    "        \n",
    "        temp_duplicates['FLUX_PSF'] = (temp_duplicates['FLUX_PSF'] + temp_duplicates['FLUX_PSF_2']) / 2\n",
    "        \n",
    "        temp_duplicates['FLUXERR_PSF'] = np.sqrt(0.25 * temp_duplicates['FLUXERR_PSF'] * temp_duplicates['FLUXERR_PSF'] + 0.25 * temp_duplicates['FLUXERR_PSF_2'] * temp_duplicates['FLUXERR_PSF_2'])\n",
    "        \n",
    "        temp_duplicates['FLUX_RADIUS'] = (temp_duplicates['FLUX_RADIUS'] + temp_duplicates['FLUX_RADIUS_2']) / 2\n",
    "        \n",
    "        temp_duplicates['ELONGATION'] = (temp_duplicates['ELONGATION'] + temp_duplicates['ELONGATION_2']) / 2\n",
    "        \n",
    "        temp_duplicates['ELLIPTICITY'] = (temp_duplicates['ELLIPTICITY'] + temp_duplicates['ELLIPTICITY_2']) / 2\n",
    "        \n",
    "        temp_duplicates['SNR_WIN'] = (temp_duplicates['SNR_WIN'] + temp_duplicates['SNR_WIN_2']) / 2\n",
    "        \n",
    "        temp_duplicates['FLAGS'] = temp_duplicates['FLAGS'] + temp_duplicates['FLAGS_2']\n",
    "        \n",
    "        temp_duplicates['FLAGS_MODEL'] = temp_duplicates['FLAGS_MODEL'] + temp_duplicates['FLAGS_MODEL_2']\n",
    "        \n",
    "        # Removes the columns from the cross-matched second catalog\n",
    "        remove_new_columns(averaged, temp_duplicates)\n",
    "        \n",
    "        averaged = vstack([singles, temp_duplicates])\n",
    "        \n",
    "        averaged = vstack([averaged, singles_cat])\n",
    "\n",
    "averaged.remove_rows([averaged['FLAGS'] != 0])\n",
    "\n",
    "ascii.write(averaged, '/home/yobd/Documents/Photometry_Output/N-A-L656_Full_Catalog_Aper_PSF.dat', format = 'csv', overwrite = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
